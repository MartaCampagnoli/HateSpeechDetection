{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartaCampagnoli/HateSpeechDetection/blob/main/No%20Output%20Notebooks/WordClouds_KeyBert_BertTopic_English_NoOutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install keyphrase-vectorizers\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "IQsWnqd9-uZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from google.colab import files\n",
        "import io\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseTfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN"
      ],
      "metadata": {
        "id": "_JR7I2oDjDFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #cleandata.csv\n",
        "df = pd.read_csv(io.BytesIO(uploaded['cleandata.csv']))"
      ],
      "metadata": {
        "id": "UPn2II-mWVkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordClouds"
      ],
      "metadata": {
        "id": "m68DM8JS9hwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"s\"])\n",
        "\n",
        "def wordcloud50(column):\n",
        "  long_string = ','.join(list(column.values))\n",
        "  wordcloud = WordCloud(background_color=\"black\", max_words=50, contour_width=3, contour_color='steelblue', width=600, height=300, stopwords = stopwords)\n",
        "  wordcloud.generate(long_string)\n",
        "  w = wordcloud.to_image()\n",
        "  return w"
      ],
      "metadata": {
        "id": "rZQQQbgB0g0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['label'] == 'hate']['text']) #hate"
      ],
      "metadata": {
        "id": "mito9U0aSOMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['label'] == 'nothate']['text'])"
      ],
      "metadata": {
        "id": "j2tcuUh_1y6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['target'] == 'wom']['text'])"
      ],
      "metadata": {
        "id": "nS5P7XMc2RoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['target'] == 'bla']['text'])"
      ],
      "metadata": {
        "id": "mUAx_mtf2iCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['target'] == 'jew']['text'])"
      ],
      "metadata": {
        "id": "tmVTP0_84Ymo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['target'] == 'mus']['text'])"
      ],
      "metadata": {
        "id": "ovT_uuMe4lzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['target'] == 'trans']['text'])"
      ],
      "metadata": {
        "id": "9QbgevSf4vkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud50(df[df['target'] == 'gay']['text'])"
      ],
      "metadata": {
        "id": "xteUJRbM43AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keybert"
      ],
      "metadata": {
        "id": "YY90Tpa-v8Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keybertextract(column):\n",
        "  long_string = ','.join(list(column.values))\n",
        "  keywords = kw_model.extract_keywords(long_string, keyphrase_ngram_range=(1, 1))\n",
        "  bigrams = kw_model.extract_keywords(long_string, keyphrase_ngram_range=(1, 2))\n",
        "  tfidf = kw_model.extract_keywords(long_string, vectorizer=KeyphraseTfidfVectorizer())\n",
        "  return keywords, bigrams, tfidf"
      ],
      "metadata": {
        "id": "zvyOVroNX2nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()"
      ],
      "metadata": {
        "id": "EhlHX9c6XxLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hatekeywords, hatebigrams, tfidfhate = keybertextract(df[df['label'] == 'hate']['text'])\n",
        "print(f\"Unigram Keywords:\", hatekeywords)\n",
        "print(f\"Bigram Keywords:\", hatebigrams)\n",
        "print(f\"TfIdf Keywords:\", tfidfhate)"
      ],
      "metadata": {
        "id": "Ylw5M94TgaLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nohatekeywords, nohatebigrams, tfidfnohate = keybertextract(df[df['label'] == 'nothate']['text'])\n",
        "print(f\"Unigram Keywords:\", nohatekeywords)\n",
        "print(f\"Bigram Keywords:\", nohatebigrams)\n",
        "print(f\"TfIdf Keywords:\", tfidfnohate)"
      ],
      "metadata": {
        "id": "hy05n8lzb3BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "womankeywords, womanbigrams, tfidfwom = keybertextract(df[df['target'] == 'wom']['text'])\n",
        "print(f\"Unigram Keywords:\", womankeywords)\n",
        "print(f\"Bigram Keywords:\", womanbigrams)\n",
        "print(f\"TfIdf Keywords:\", tfidfwom)"
      ],
      "metadata": {
        "id": "E8WujLf-3LVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blackpeoplekeywords, blackpeoplebigram, tfidfblackpeople = keybertextract(df[df['target'] == 'bla']['text'])\n",
        "print(f\"Unigram Keywords:\", blackpeoplekeywords)\n",
        "print(f\"Bigram Keywords:\", blackpeoplebigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfblackpeople)"
      ],
      "metadata": {
        "id": "qumIOclpcUmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jewishpeoplekeywords, jewishpeoplebigram, tfidfjewish = keybertextract(df[df['target'] == 'jew']['text'])\n",
        "print(f\"Unigram Keywords:\", jewishpeoplekeywords)\n",
        "print(f\"Bigram Keywords:\", jewishpeoplebigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfjewish)"
      ],
      "metadata": {
        "id": "fdVmiaLmZoBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "muslimpeoplekeywords, muslimpeoplebigram, tfidfmuslim = keybertextract(df[df['target'] == 'mus']['text'])\n",
        "print(f\"Unigram Keywords:\", muslimpeoplekeywords)\n",
        "print(f\"Bigram Keywords:\", muslimpeoplebigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfmuslim)"
      ],
      "metadata": {
        "id": "2H8FFVDfZrA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transpeoplekeywords, transpeoplebigram, tfidftranspeople = keybertextract(df[df['target'] == 'trans']['text'])\n",
        "print(f\"Unigram Keywords:\", transpeoplekeywords)\n",
        "print(f\"Bigram Keywords:\", transpeoplebigram)\n",
        "print(f\"TfIdf Keywords:\", tfidftranspeople)"
      ],
      "metadata": {
        "id": "eLPsRzDIdp5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gaypeoplekeywords, gaypeoplebigram, tfidfgaypeople = keybertextract(df[df['target'] == 'gay']['text'])\n",
        "print(f\"Unigram Keywords:\", gaypeoplekeywords)\n",
        "print(f\"Bigram Keywords:\", gaypeoplebigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfgaypeople)"
      ],
      "metadata": {
        "id": "2VaqKc_Ndqts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BerTopic"
      ],
      "metadata": {
        "id": "JmaL15JCIejG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hatenew = hate.groupby('target', group_keys=False).apply(lambda x: x.sample(frac=0.2)) #resample proportionally to target class, saved locally"
      ],
      "metadata": {
        "id": "pPp7Bp2_K6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #hatenew.csv\n",
        "hatenew = pd.read_csv(io.BytesIO(uploaded['hatenew.csv']))"
      ],
      "metadata": {
        "id": "9gkawSgo6ltN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = hatenew['text'].values.tolist()"
      ],
      "metadata": {
        "id": "IJzFLNrT7nbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sent_tokenize(piece) for piece in data]\n",
        "sentences = [sentence for doc in sentences for sentence in doc]\n",
        "\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(sentences, show_progress_bar=True)\n",
        "\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=50, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ],
      "metadata": {
        "id": "H-YAxbpWAS8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  vectorizer_model=KeyphraseTfidfVectorizer(),\n",
        "\n",
        "  # Hyperparameters\n",
        "  top_n_words=10,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "topicsnew, probsnew = topic_model_new.fit_transform(sentences, embeddings)"
      ],
      "metadata": {
        "id": "TCcaCvwEBlpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show topics\n",
        "topic_model_new.get_topic_info()"
      ],
      "metadata": {
        "id": "S5MAPL53WJ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = topic_model_new.generate_topic_labels(nr_words=1,topic_prefix=False,word_length=10,separator=\", \")\n",
        "topic_model_new.set_topic_labels(topic_labels)"
      ],
      "metadata": {
        "id": "OPuirfJ-dJqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new.visualize_barchart(n_words=10, width=300, height=300, top_n_topics=14, custom_labels= True)"
      ],
      "metadata": {
        "id": "VdDx-4uICPQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new.visualize_topics()"
      ],
      "metadata": {
        "id": "EVz7DuOoeFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_topics, similarity = topic_model_new.find_topics(\"woman\", top_n=5)\n",
        "topic_model_new.get_topic(similar_topics[0])"
      ],
      "metadata": {
        "id": "I7GNUCuKe3Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new.save(\"my_model_3\", serialization=\"pickle\")"
      ],
      "metadata": {
        "id": "we7b_7l_eVU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
