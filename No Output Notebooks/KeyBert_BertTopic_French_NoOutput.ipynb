{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartaCampagnoli/HateSpeechDetection/blob/main/No%20Output%20Notebooks/KeyBert_BertTopic_French_NoOutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install keyphrase-vectorizers\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "IQsWnqd9-uZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "from google.colab import files\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseTfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "_JR7I2oDjDFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('french')"
      ],
      "metadata": {
        "id": "xoo3YTBTV1yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #fr_dataset.csv"
      ],
      "metadata": {
        "id": "UPn2II-mWVkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded['fr_dataset.csv']))"
      ],
      "metadata": {
        "id": "cvCvBnihQzmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = re.sub(' RT ', '', text)\n",
        "    text = re.sub('RT', '', text)\n",
        "    text = re.sub('rt', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "df['tweet'] = df['tweet'].astype(str).apply(clean_tweet)\n",
        "df['tweet'] = df['tweet'].astype(str).apply(preprocess)"
      ],
      "metadata": {
        "id": "41BbDw1EP4Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keybert"
      ],
      "metadata": {
        "id": "YY90Tpa-v8Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keybertextract(column):\n",
        "  long_string = ','.join(list(column.values))\n",
        "  keywords = kw_model.extract_keywords(long_string, keyphrase_ngram_range=(1, 1))\n",
        "  bigrams = kw_model.extract_keywords(long_string, keyphrase_ngram_range=(1, 2))\n",
        "  tfidf = kw_model.extract_keywords(long_string, vectorizer=KeyphraseTfidfVectorizer())\n",
        "  return keywords, bigrams, tfidf"
      ],
      "metadata": {
        "id": "zvyOVroNX2nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()"
      ],
      "metadata": {
        "id": "EhlHX9c6XxLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genkeywords, genbigrams, tfidfgen = keybertextract(df['tweet'])\n",
        "print(f\"Unigram Keywords:\", genkeywords)\n",
        "print(f\"Bigram Keywords:\", genbigrams)\n",
        "print(f\"TfIdf Keywords:\", tfidfgen)"
      ],
      "metadata": {
        "id": "Tk16VbP2XwDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indkeywords, indbigram, tfidfind = keybertextract(df[df['group'] == 'individual']['tweet'])\n",
        "print(f\"Unigram Keywords:\", indkeywords)\n",
        "print(f\"Bigram Keywords:\", indbigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfind)"
      ],
      "metadata": {
        "id": "eLPsRzDIdp5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "othkeywords, othbigram, tfidfoth = keybertextract(df[df['group'] == 'other']['tweet'])\n",
        "print(f\"Unigram Keywords:\", othkeywords)\n",
        "print(f\"Bigram Keywords:\", othbigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfoth)"
      ],
      "metadata": {
        "id": "2VaqKc_Ndqts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adkeywords, adbigram, tfidfad = keybertextract(df[df['group'] == 'african_descent']['tweet'])\n",
        "print(f\"Unigram Keywords:\", adkeywords)\n",
        "print(f\"Bigram Keywords:\", adbigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfad)"
      ],
      "metadata": {
        "id": "qumIOclpcUmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arkeywords, arbigram, tfidfar = keybertextract(df[df['group'] == 'arabs']['tweet'])\n",
        "print(f\"Unigram Keywords:\", arkeywords)\n",
        "print(f\"Bigram Keywords:\", arbigram)\n",
        "print(f\"TfIdf Keywords:\", tfidfar)"
      ],
      "metadata": {
        "id": "2H8FFVDfZrA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BerTopic: suggested pipeline"
      ],
      "metadata": {
        "id": "ojBRwCxxWwmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = df['tweet'].values.tolist()"
      ],
      "metadata": {
        "id": "IJzFLNrT7nbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sent_tokenize(piece) for piece in data]\n",
        "sentences = [sentence for doc in sentences for sentence in doc]\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(sentences, show_progress_bar=True)\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=50, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ],
      "metadata": {
        "id": "H-YAxbpWAS8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  vectorizer_model=KeyphraseTfidfVectorizer(),\n",
        "\n",
        "  # Hyperparameters\n",
        "  top_n_words=10,\n",
        "  verbose=True,\n",
        "  language = 'french'\n",
        ")\n",
        "\n",
        "# Train model\n",
        "topicsnew, probsnew = topic_model_new.fit_transform(sentences, embeddings)"
      ],
      "metadata": {
        "id": "TCcaCvwEBlpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show topics\n",
        "topic_model_new.get_topic_info()"
      ],
      "metadata": {
        "id": "S5MAPL53WJ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = topic_model_new.generate_topic_labels(nr_words=1,topic_prefix=False,word_length=10,separator=\", \")\n",
        "topic_model_new.set_topic_labels(topic_labels)"
      ],
      "metadata": {
        "id": "OPuirfJ-dJqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new.visualize_barchart(n_words=8, width=500, height=500, top_n_topics=11, custom_labels= True)"
      ],
      "metadata": {
        "id": "VdDx-4uICPQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_new.visualize_topics()"
      ],
      "metadata": {
        "id": "EVz7DuOoeFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_topics, similarity = topic_model_new.find_topics(\"woman\", top_n=5)\n",
        "topic_model_new.get_topic(similar_topics[0])"
      ],
      "metadata": {
        "id": "I7GNUCuKe3Ha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}